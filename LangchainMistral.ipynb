{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-d-abLEVi7e"
      },
      "source": [
        "# Introduction to LangChain\n",
        "\n",
        "- https://console.mistral.ai/\n",
        "- https://www.langchain.com/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tkjUUDWqMuMx",
        "outputId": "23a0ccba-6aee-4734-bae6-2adc24e367a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret mistralapikey does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8a97a827c8b0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the API key here and add it to the secrets (left).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mistralapikey\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret mistralapikey does not exist."
          ]
        }
      ],
      "source": [
        "# Get the API key here and add it to the secrets (left).\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get(\"mistralapikey\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaIXwH0zQKj_"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain-core langchain-mistralai langchain-community langchain-chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll_ToC8h5qxm"
      },
      "source": [
        "# Imports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G87WPagmQipG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "\n",
        "from langchain_core.globals import set_verbose, set_debug\n",
        "set_verbose(False)\n",
        "set_debug(False)\n",
        "\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kQVM2DC-5AZ"
      },
      "source": [
        "## Getting started.\n",
        "\n",
        "- https://docs.mistral.ai/getting-started/models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ILPGkwUQn-Z"
      },
      "outputs": [],
      "source": [
        "llm = ChatMistralAI(\n",
        "    api_key=api_key,\n",
        "    model=\"mistral-medium-latest\"\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\n",
        "        content=\"You are a friendly AI assistant that speaks English but sometimes uses German words.\"\n",
        "    ),\n",
        "    HumanMessage(\n",
        "        content=\"Write a poem about love.\"\n",
        "    )\n",
        "]\n",
        "result = llm.invoke(messages)\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJsAgzcABH5F"
      },
      "outputs": [],
      "source": [
        "print(json.dumps(result.response_metadata, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emLJ8nkc_qQw"
      },
      "source": [
        "## Use streaming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_B5PHfs-4kX"
      },
      "outputs": [],
      "source": [
        "async for chunk in llm.astream(messages):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0e1F1TWHt5M"
      },
      "source": [
        "## Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44W0B6uZTpa4"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    SystemMessage(\n",
        "        content=\"\"\n",
        "            \"You are a friendly AI assistant.\"\n",
        "            \" Your specialty are great translations. Answer with the translation first. And then explain it in detail. Explanation as a bulleted list please. Use HTML tags.\"\n",
        "    ),\n",
        "    HumanMessage(\n",
        "        content=\"\"\n",
        "            \"L'homme est libre au moment qu'il veut l'être.\"\n",
        "        )\n",
        "]\n",
        "result = llm.invoke(messages)\n",
        "result.content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.content)"
      ],
      "metadata": {
        "id": "eYQ0v_5OlK3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKy5M8CwU9R5"
      },
      "source": [
        "## Chain example: Parsing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtMwq7dSU_cL"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "result = llm.invoke(messages)\n",
        "print(result)\n",
        "parsed_result = parser.invoke(result)\n",
        "print(parsed_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91XgS0i9VQgm"
      },
      "outputs": [],
      "source": [
        "chain = llm | parser\n",
        "chain.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ielLqj5kpX"
      },
      "source": [
        "## Summarization.\n",
        "\n",
        "https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSyd4Tjl8BIJ"
      },
      "outputs": [],
      "source": [
        "# Get a file.\n",
        "!wget https://raw.githubusercontent.com/vilmibm/lovecraftcorpus/master/ulthar.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXNmf09G8C9n"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"ulthar.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "first_document_content = documents[0].page_content\n",
        "\n",
        "summary_prompt = f\"Please summarize the following document:\\n\\n{first_document_content}\"\n",
        "messages = [\n",
        "    SystemMessage(\n",
        "        content=\"You are a friendly AI assistant that speaks English.\"\n",
        "                \"You write really good summaries.\"\n",
        "                \"You sometimes use bulleted lists but not all the time.\"\n",
        "    ),\n",
        "    HumanMessage(content=summary_prompt)\n",
        "]\n",
        "\n",
        "chain = llm | parser\n",
        "summary = chain.invoke(messages)\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oddbPE6sgfud"
      },
      "source": [
        "## Advanced summarization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUX7Mt06gfBz"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "summary = chain.invoke(documents)[\"output_text\"]\n",
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx0XsuIKhMe_"
      },
      "outputs": [],
      "source": [
        "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "summary = chain.invoke(documents)[\"output_text\"]\n",
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FKZ_gnIhOFu"
      },
      "outputs": [],
      "source": [
        "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
        "summary = chain.invoke(documents)[\"output_text\"]\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVF7Yc-lAtK7"
      },
      "source": [
        "## Structured output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO-Os0Ao_8Hz"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\n",
        "        content=\"You are a friendly AI assistant that speaks English.\"\n",
        "                \"Your specialty is extracting structured output in JSON.\"\n",
        "    ),\n",
        "    HumanMessage(\n",
        "        content=f\"List all the characters and what you know about them as JSON:\\n\\n{first_document_content}\"\n",
        "    )\n",
        "]\n",
        "\n",
        "chain = llm | JsonOutputParser()\n",
        "structured_output = chain.invoke(messages)\n",
        "print(json.dumps(structured_output, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvNLqeDtiNwV"
      },
      "source": [
        "## Advanced structured output with Pydantic\n",
        "\n",
        "- https://docs.pydantic.dev/latest/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUYWTaGLiTax"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "from langchain.chains import create_extraction_chain_pydantic\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    first_name: str\n",
        "    last_name: str\n",
        "    known_facts: str\n",
        "\n",
        "class PersonGroup(BaseModel):\n",
        "    persons: List[Person]\n",
        "\n",
        "llm_small = ChatMistralAI(\n",
        "    api_key=api_key,\n",
        "    model=\"mistral-small-latest\" # Medium does not have function calling.\n",
        ")\n",
        "\n",
        "chain = llm_small.with_structured_output(PersonGroup)\n",
        "structured_output = chain.invoke(summary_prompt)\n",
        "print(structured_output)\n",
        "print()\n",
        "print(structured_output.model_dump_json(indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dnb95WT4gpF"
      },
      "source": [
        "## Classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EAqmg8B4hFx"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Classification(BaseModel):\n",
        "    sentiment: str = Field(\n",
        "        ...,\n",
        "        description=\"describes the sentiment of the statement\",\n",
        "        enum=[\"negative\", \"neutral\", \"positive\"]\n",
        "    )\n",
        "    aggressiveness: int = Field(\n",
        "        ...,\n",
        "        description=\"describes how aggressive the statement is, the higher the number the more aggressive\",\n",
        "        enum=[0, 1, 2, 3],\n",
        "    )\n",
        "    language: str = Field(\n",
        "        ...,\n",
        "        description=\"describes the language of the statement\",\n",
        "        enum=[\"english\", \"french\", \"german\", \"other\"]\n",
        "    )\n",
        "\n",
        "statements = [\n",
        "    \"I absolutely love this new restaurant! The food is amazing, and the service is top-notch.\",\n",
        "    \"Le service client ici est terrible, et je ne reviendrai jamais.\",\n",
        "    \"Ich bin gleichgültig gegenüber der neuen Politik; sie betrifft mich nicht wirklich.\",\n",
        "    \"Your recent actions were completely unacceptable, and they have consequences.\",\n",
        "    \"Quel beau jour ! Je me sens si heureux et en paix.\",\n",
        "    \"Die Art und Weise, wie Sie die Situation gehandhabt haben, war sehr enttäuschend und unprofessionell.\",\n",
        "    \"Creo que la presentación estuvo bien, pero podría mejorar.\",\n",
        "    \"You have no right to speak to me that way! It's utterly disrespectful.\",\n",
        "    \"Ce livre est très intéressant, et j'ai beaucoup aimé le lire.\",\n",
        "    \"Ihre Bemühungen bei dem Projekt waren bestenfalls mittelmäßig, und wir müssen das besprechen.\",\n",
        "    \"Hab SoSlI' Quch!\"\n",
        "]\n",
        "\n",
        "for statement in statements:\n",
        "    chain = llm_small.with_structured_output(Classification)\n",
        "    structured_output = chain.invoke(statement)\n",
        "    print(statement)\n",
        "    print(structured_output.dict())\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRHii9sOCE1V"
      },
      "source": [
        "## Tool use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaF0q6CU9YGE"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def sum_tool(numbers:list) -> int:\n",
        "    \"\"\"Sum up numbers.\"\"\"\n",
        "    return sum(numbers)\n",
        "\n",
        "print(sum_tool.name)\n",
        "print(sum_tool.description)\n",
        "print(sum_tool.args)\n",
        "\n",
        "numbers = [42, 308423, 666, 1000000, 1729, -1245, 768]\n",
        "numbers_string = \", \".join(str(n) for n in numbers)\n",
        "\n",
        "# Sanity.\n",
        "print(\"Expected:\", sum(numbers))\n",
        "print(\"\")\n",
        "\n",
        "# Create the prompts.\n",
        "system_prompt = f\"You are a friendly AI assistant that speaks English. You are good at math.\"\n",
        "sum_prompt = f\"Please sum up the following numbers: {numbers_string}.\"\n",
        "messages = [\n",
        "    SystemMessage(content=sum_prompt),\n",
        "    HumanMessage(content=numbers_string)\n",
        "]\n",
        "\n",
        "# Without tools.\n",
        "print(\"Without tools:\")\n",
        "print(llm_small.invoke(messages))\n",
        "print(\"\")\n",
        "\n",
        "# With tools.\n",
        "print(\"With tools:\")\n",
        "llm_small_with_tools = llm_small.bind_tools([sum_tool])\n",
        "print(llm_small_with_tools.invoke(messages))\n",
        "chain = llm_small_with_tools | (lambda x: x.tool_calls[0][\"args\"]) | sum_tool\n",
        "print(chain.invoke(messages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K59VlZkmDXkW"
      },
      "source": [
        "## Loading PDFs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQCVnkO88Inu"
      },
      "outputs": [],
      "source": [
        "!wget https://www.pileface.com/sollers/pdf/Zarathustra.pdf\n",
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H68EULy6Da4W"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"Zarathustra.pdf\")\n",
        "pages = loader.load_and_split()\n",
        "\n",
        "for page in pages[:2]:\n",
        "    print(page.page_content)\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhcpTXk3F3w3"
      },
      "source": [
        "## Web Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J6Q1o87DrFZ"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://de.wikipedia.org/wiki/Heilbronn\")\n",
        "\n",
        "pages = loader.load_and_split()\n",
        "\n",
        "for page in pages[:2]:\n",
        "    print(page.page_content)\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmb2CaNrHMtj"
      },
      "source": [
        "## Gradio chat.\n",
        "\n",
        "https://www.gradio.app/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6mikqJhHMM9"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G1JjTQhQlYTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFolpDDPFu9c"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def predict(message, history):\n",
        "    history_langchain_format = []\n",
        "    for human, ai in history:\n",
        "        history_langchain_format.append(HumanMessage(content=human))\n",
        "        history_langchain_format.append(AIMessage(content=ai))\n",
        "    history_langchain_format.append(HumanMessage(content=message))\n",
        "    gpt_response = llm(history_langchain_format)\n",
        "    return gpt_response.content\n",
        "\n",
        "gr.ChatInterface(predict).launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9b7VMDSOheU"
      },
      "source": [
        "## Vector Databases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fpam-59rp6I9"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_mistralai import MistralAIEmbeddings\n",
        "\n",
        "embeddings_model = MistralAIEmbeddings(\n",
        "    api_key=api_key,\n",
        "    model=\"mistral-embed\"\n",
        ")\n",
        "\n",
        "embedding = embeddings_model.embed_query(\"This is a test, I want to embed.\")\n",
        "print(len(embedding))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Hsb8C5crPTb"
      },
      "outputs": [],
      "source": [
        "from langchain.evaluation import load_evaluator\n",
        "\n",
        "evaluator = load_evaluator(\"embedding_distance\", embeddings=embeddings_model)\n",
        "\n",
        "distance = evaluator.evaluate_strings(\n",
        "    prediction=\"Dune is a great movie.\",\n",
        "    reference=\"I like the Star Wars series.\"\n",
        ")\n",
        "print(distance)\n",
        "\n",
        "distance = evaluator.evaluate_strings(\n",
        "    prediction=\"Dune is a great movie.\",\n",
        "    reference=\"Hi. I am Tristan. I love teaching AI.\"\n",
        ")\n",
        "print(distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j08pQvBJsJsS"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/vilmibm/lovecraftcorpus/master/mountains_of_madness.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apilPYnlOnEx"
      },
      "source": [
        "## Let us use Chroma.\n",
        "\n",
        "- https://www.trychroma.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqEdZIrfsR3l"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "raw_documents = TextLoader(\"mountains_of_madness.txt\").load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "documents = text_splitter.split_documents(raw_documents)\n",
        "print(f\"Got {len(documents)} documents after splitting\")\n",
        "\n",
        "print(documents[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill the database."
      ],
      "metadata": {
        "id": "1mBa3vw6wjtD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDVUM0OitQ4Z"
      },
      "outputs": [],
      "source": [
        "database = Chroma.from_documents(documents, embeddings_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query the database."
      ],
      "metadata": {
        "id": "K8UesY9WwkKG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c_oWN_pttFG"
      },
      "outputs": [],
      "source": [
        "query = \"What is an Old One?\"\n",
        "docs = database.similarity_search(query)\n",
        "docs[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDK-NSjwv6g0"
      },
      "outputs": [],
      "source": [
        "query = \"What is an Old One?\"\n",
        "docs = database.similarity_search_with_score(query)\n",
        "docs[0][0].page_content, docs[0][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDfAAoyw0fjg"
      },
      "source": [
        "## Talk to document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MVGscxPyJFl"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "qa_chain = load_qa_chain(llm)\n",
        "\n",
        "template=\"\"\"Given the following conversation history and a new user question, generate a standalone question.\n",
        "Conversation history:\n",
        "{chat_history}\n",
        "New question: {question}\n",
        "Standalone question:\"\"\"\n",
        "\n",
        "question_generator_prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"question\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "question_generator_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=question_generator_prompt\n",
        ")\n",
        "\n",
        "retrieval_chain = ConversationalRetrievalChain(\n",
        "    retriever=database.as_retriever(search_kwargs={\"k\": 5}),\n",
        "    combine_docs_chain=qa_chain,\n",
        "    question_generator=question_generator_chain\n",
        ")\n",
        "\n",
        "def predict(message, history):\n",
        "    history_langchain_format = []\n",
        "    for human, ai in history:\n",
        "        history_langchain_format.append(HumanMessage(content=human))\n",
        "        history_langchain_format.append(AIMessage(content=ai))\n",
        "\n",
        "    history_langchain_format.append(HumanMessage(content=message))\n",
        "\n",
        "    response = retrieval_chain(\n",
        "        {\"question\": message, \"chat_history\": history_langchain_format}\n",
        "    )\n",
        "\n",
        "    return response[\"answer\"]\n",
        "\n",
        "gr.ChatInterface(predict).launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Software development"
      ],
      "metadata": {
        "id": "1bgn3iOLu74H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(\n",
        "        content=\"You are a 150K EUR/year principal software engineer. You write the best code in the world.\"\n",
        "    ),\n",
        "    HumanMessage(\n",
        "        content=\"Implement Conway's game of life in Python.\"\n",
        "    )\n",
        "]\n",
        "result = llm.invoke(messages)\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "aBLkgFl6vHx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code = \"\"\"\n",
        "class ToDoList:\n",
        "    def __init__(self):\n",
        "        self.tasks = []\n",
        "\n",
        "    def add_task(self, task: str):\n",
        "        if not isinstance(task, str) or not task.strip():\n",
        "            raise ValueError(\"Task must be a non-empty string\")\n",
        "        self.tasks.append({\"task\": task, \"completed\": False})\n",
        "\n",
        "    def remove_task(self, task: str):\n",
        "        for t in self.tasks:\n",
        "            if t[\"task\"] == task:\n",
        "                self.tasks.remove(t)\n",
        "                return\n",
        "        raise ValueError(\"Task not found\")\n",
        "\n",
        "    def mark_completed(self, task: str):\n",
        "        for t in self.tasks:\n",
        "            if t[\"task\"] == task:\n",
        "                t[\"completed\"] = True\n",
        "                return\n",
        "        raise ValueError(\"Task not found\")\n",
        "\n",
        "    def get_tasks(self, completed=None):\n",
        "        if completed is None:\n",
        "            return self.tasks\n",
        "        return [t for t in self.tasks if t[\"completed\"] == completed]\n",
        "\n",
        "    def clear_completed(self):\n",
        "        self.tasks = [t for t in self.tasks if not t[\"completed\"]]\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\n",
        "        content=\"You are a 150K EUR/year principal software engineer. You write the best code in the world.\"\n",
        "    ),\n",
        "    HumanMessage(\n",
        "        content=f\"Here is some code:\\n\\n'''\\n{code}\\n'''\\n\\nPlease write unit tests.\"\n",
        "    )\n",
        "]\n",
        "result = llm.invoke(messages)\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "detX5NYyvE5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(\n",
        "        content=\"You are a 150K EUR/year principal code reviewer. You write code reviews even god has never seen.\"\n",
        "    ),\n",
        "    HumanMessage(\n",
        "        content=f\"Here is some code:\\n\\n'''\\n{code}\\n'''\\n\\nPlease a code review..\"\n",
        "    )\n",
        "]\n",
        "result = llm.invoke(messages)\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "Jg9zhCP5vlqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr9pB0pP3eQF"
      },
      "source": [
        "TODO: https://blog.langchain.dev/reflection-agents/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}